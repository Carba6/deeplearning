{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvsyzutm00N6+vp73MigfG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Carba6/deeplearning/blob/main/Wide_Resnet_28_10_Relu6_2bit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ccdb04N-Aq2O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 898
        },
        "outputId": "6166e628-b91f-4882-dcee-040c1cad2832"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:15<00:00, 11069656.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Wide-Resnet 28x10\n",
            "Is GPU available? True\n",
            "Current device: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torch/ao/quantization/fake_quantize.py:342: UserWarning: _aminmax is deprecated as of PyTorch 1.11 and will be removed in a future release. Use aminmax instead. This warning will only appear once per process. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:651.)\n",
            "  return torch.fused_moving_avg_obs_fake_quant(\n",
            "/usr/local/lib/python3.9/dist-packages/torch/ao/quantization/fake_quantize.py:342: UserWarning: _aminmax is deprecated as of PyTorch 1.11 and will be removed in a future release. Use aminmax instead. This warning will only appear once per process. (Triggered internally at ../aten/src/ATen/native/ReduceAllOps.cpp:65.)\n",
            "  return torch.fused_moving_avg_obs_fake_quant(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 1.6696, Test Accuracy: 34.64%\n",
            "Epoch: 2, Loss: 1.1788, Test Accuracy: 52.50%\n",
            "Epoch: 3, Loss: 0.9435, Test Accuracy: 64.58%\n",
            "Epoch: 4, Loss: 0.8143, Test Accuracy: 67.00%\n",
            "Epoch: 5, Loss: 0.7024, Test Accuracy: 73.13%\n",
            "Epoch: 6, Loss: 0.6180, Test Accuracy: 74.58%\n",
            "Epoch: 7, Loss: 0.5517, Test Accuracy: 79.61%\n",
            "Epoch: 8, Loss: 0.5118, Test Accuracy: 77.60%\n",
            "Epoch: 9, Loss: 0.4755, Test Accuracy: 81.05%\n",
            "Epoch: 10, Loss: 0.4822, Test Accuracy: 80.73%\n",
            "Epoch: 11, Loss: 0.4669, Test Accuracy: 78.28%\n",
            "Epoch: 12, Loss: 0.4339, Test Accuracy: 79.08%\n",
            "Epoch: 13, Loss: 0.4248, Test Accuracy: 78.75%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d434bbdae462>\u001b[0m in \u001b[0;36m<cell line: 114>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-d434bbdae462>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;31m# 训练循环\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqat_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqat_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-d434bbdae462>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrunning_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from wide_resnet import Wide_ResNet\n",
        "import torch.quantization as quantization\n",
        "from torch.quantization import FakeQuantize, default_qconfig, QuantStub, DeQuantStub\n",
        "from torch.quantization.qconfig import QConfig\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    batch_size = 128\n",
        "    learning_rate = 0.1\n",
        "    epochs = 200\n",
        "    weight_decay = 0.0005\n",
        "    momentum = 0.9\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    num_bits = 2\n",
        "\n",
        "    # 数据预处理\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "    # 加载CIFAR-10数据集\n",
        "    train_dataset = datasets.CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\n",
        "    test_dataset = datasets.CIFAR10(root=\"./data\", train=False, transform=transform, download=True)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "    # 创建Wide ResNet模型\n",
        "    model = Wide_ResNet(depth=28, widen_factor=10, num_classes=10, dropout_rate=0.0).to(device)\n",
        "\n",
        "    # 创建5位量化配置\n",
        "    five_bit_qconfig = QConfig(\n",
        "        activation=FakeQuantize.with_args(observer=torch.quantization.MinMaxObserver, dtype=torch.quint8, qscheme=torch.per_tensor_affine, num_bits=num_bits),\n",
        "        weight=FakeQuantize.with_args(observer=torch.quantization.MinMaxObserver, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, num_bits=num_bits)\n",
        "    )\n",
        "\n",
        "    # 准备QAT\n",
        "    model.qconfig = quantization.get_default_qat_qconfig('fbgemm')\n",
        "    qat_model = quantization.prepare_qat(model, inplace=False).to(device)\n",
        "\n",
        "    # 设置损失函数和优化器\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(qat_model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "    # 设置学习率调度器\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[60, 120], gamma=0.1)\n",
        "\n",
        "    # 训练和测试函数\n",
        "    def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "        return running_loss / len(dataloader)\n",
        "\n",
        "\n",
        "    def test(model, dataloader, criterion, device):\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in dataloader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += targets.size(0)\n",
        "                correct += (predicted == targets).sum().item()\n",
        "        return correct / total\n",
        "\n",
        "\n",
        "    # 训练循环\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train_loss = train_epoch(qat_model, train_loader, criterion, optimizer, device)\n",
        "        test_accuracy = test(qat_model, test_loader, criterion, device)\n",
        "        scheduler.step()\n",
        "        print(f\"Epoch: {epoch}, Loss: {train_loss:.4f}, Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "        # 每隔一定的epoch数量（如5个），冻结统计数据并执行一次量化\n",
        "        if epoch % 5 == 4:\n",
        "            qat_model.apply(quantization.disable_observer)\n",
        "            qat_model.apply(quantization.enable_fake_quant)\n",
        "\n",
        "        # 每隔一定的epoch数量（如10个），解冻统计数据\n",
        "        if epoch % 10 == 9:\n",
        "            qat_model.apply(quantization.enable_observer)\n",
        "\n",
        "    # 量化训练完成后，将QAT模型转换为量化模型\n",
        "    quantized_model = quantization.convert(qat_model, inplace=False)\n",
        "    \n",
        "    # 保存最终量化模型\n",
        "    quantized_model_path = f\"WRN_Relu6_{num_bits}bit.pth\"\n",
        "    torch.save(quantized_model.state_dict(), quantized_model_path)\n",
        "    print(f\"{num_bits}-bit Relu6 Quantized WRN model saved as {quantized_model_path}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    }
  ]
}